import os
import pickle
import time
import ast
import logging
from datetime import datetime
import pandas as pd
from openai import OpenAI

def get_chat_completion(client, prompt, model="deepseek-chat", json_mode=True, max_tokens=1200):
    """
    This function is used to call the OpenAI client to obtain the chat completion result.

    Args:
        client: The new OpenAI client object (new in the OpenAI 1.x version), used to interact with the OpenAI service.
        prompt: The prompt information to be completed, which serves as the text content input to the model.
        model: Used to specify the model to be used, with a default value of "deepseek-chat".
        json_mode: A boolean value indicating whether to return the response in JSON format. This is a new feature in the OpenAI 1.x version, and the default is True.
        max_tokens: An integer specifying the maximum number of tokens to be generated by the model, with a default value of 1200.

    Returns:
        The completion result generated by the model, which may be a JSON object or a normal response depending on the value of the json_mode parameter.
    """
    # Build a message list to wrap the prompt information in a format that meets OpenAI's requirements.
    messages = [{"role": "user", "content": prompt}]
    if json_mode:
        # If the response needs to be returned in JSON format
        completion = client.chat.completions.create(
            model=model,
            # Specify the response format as a JSON object
            response_format={"type": "json_object"},
            messages=messages,
            # Set the temperature parameter to 0 to make the model's output more deterministic and reduce randomness
            temperature=0,
            # Limit the maximum number of tokens generated by the model
            max_tokens=max_tokens
        )
    else:
        # If the response does not need to be returned in JSON format
        completion = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0,
            max_tokens=max_tokens
        )
    return completion

def load_dataset(dataset_name):
    """
    Load the training, validation, and test datasets. Merge the training and validation datasets,
    and perform necessary sorting and data type conversion.

    Args:
        dataset_name (str): The name of the dataset.

    Returns:
        tuple: A tuple containing the merged training and validation data (as a Pandas DataFrame)
               and the test data (as a list of dictionaries).
    """
    # Load the training and validation datasets from CSV files
    training_data = pd.read_csv(f"data/{dataset_name}/{dataset_name}_train.csv")
    validation_data = pd.read_csv(f"data/{dataset_name}/{dataset_name}_valid.csv")

    # Load the test dataset from a pickle file
    test_dataset_path = f"data/{dataset_name}/{dataset_name}_testset.pk"
    with open(test_dataset_path, "rb") as file:
        test_dataset = pickle.load(file)

    # Combine the training and validation datasets
    tv_dataset = pd.concat([training_data, validation_data], ignore_index=True)

    # Sort the combined dataset by user ID, start day, and start minute
    sorting_columns = ['user_id', 'start_day', 'start_min']
    tv_dataset.sort_values(sorting_columns, inplace=True)

    # Convert the 'duration' column to integer type if the dataset is 'geolife'
    if dataset_name == 'geolife':
        tv_dataset['duration'] = tv_dataset['duration'].astype(int)

    # Print the number of test samples
    print(f"Number of total test samples: {len(test_dataset)}")

    return tv_dataset, test_dataset

def convert_minutes_to_12_hour_format(total_minutes):
    """
    Convert the given total number of minutes into a 12-hour clock time format.

    Args:
        total_minutes (int): The total number of minutes to be converted.

    Returns:
        str: A string representing the time in 12-hour clock format (e.g., "03:45 PM"),
             or an error message if the input is invalid.
    """
    # Validate the input minutes
    if total_minutes < 0 or total_minutes >= 1440:
        return "Invalid input. Minutes should be between 0 and 1439."

    # Calculate the number of hours and remaining minutes
    hour_count = total_minutes // 60
    remaining_minutes = total_minutes % 60

    # Determine the time period (AM or PM)
    time_period = "AM"
    if hour_count >= 12:
        time_period = "PM"

    # Adjust the hour for 12-hour clock representation
    if hour_count == 0:
        hour_count = 12
    elif hour_count > 12:
        hour_count -= 12

    # Format the output string
    return f"{hour_count:02d}:{remaining_minutes:02d} {time_period}"

def integer_to_weekday(day_integer):
    """
    Convert an integer representing a day of the week (0 - 6) to its corresponding weekday name.

    Args:
        day_integer (int): An integer from 0 to 6, where 0 represents Monday and 6 represents Sunday.

    Returns:
        str: The name of the weekday corresponding to the given integer.
    """
    weekday_mapping = {
        0: 'Monday',
        1: 'Tuesday',
        2: 'Wednesday',
        3: 'Thursday',
        4: 'Friday',
        5: 'Saturday',
        6: 'Sunday'
    }
    return weekday_mapping[day_integer]

def setup_logger(logger_identifier, log='logs/'):
    """
    Set up a logger with both console and file handlers.

    Args:
        logger_identifier (str): A unique identifier for the logger.
        log (str): The directory where log files will be stored. Defaults to 'logs/'.

    Returns:
        logging.Logger: A configured logger instance.
    """
    # Check if the log directory exists; if not, create it
    if not os.path.exists(log):
        os.makedirs(log)

    # Initialize a logger with the given identifier
    configured_logger = logging.getLogger(logger_identifier)
    configured_logger.setLevel(logging.DEBUG)

    # Create a console handler to display logs on the console
    console_log_handler = logging.StreamHandler()
    console_log_handler.setLevel(logging.DEBUG)

    # Generate a timestamped log file name
    current_time = datetime.now()
    timestamp = current_time.strftime("%Y%m%d_%H%M%S")
    log_file_name = f'log_file_{timestamp}.log'
    log_file_full_path = os.path.join(log, log_file_name)

    # Create a file handler to write logs to the file
    file_log_handler = logging.FileHandler(log_file_full_path)
    file_log_handler.setLevel(logging.DEBUG)

    # Define a log message formatter
    log_message_formatter = logging.Formatter('%(message)s')
    console_log_handler.setFormatter(log_message_formatter)
    file_log_handler.setFormatter(log_message_formatter)

    # Attach the console and file handlers to the logger
    configured_logger.addHandler(console_log_handler)
    configured_logger.addHandler(file_log_handler)

    return configured_logger

def get_user_data(train_dataset, uid, No_Recent_Trajectory, logger):
    user_train = train_dataset[train_dataset['user_id'] == uid]
    logger.info(f"Length of user {uid} train data: {len(user_train)}")
    user_train = user_train.tail(No_Recent_Trajectory)
    logger.info(f"Number of user historical stays: {len(user_train)}")
    return user_train

def organise_data(dataset_name, user_train, test_dataset, uid, logger, Recent_Trajectory=10):
    """
    This function is used to organize data in another way. It processes the training data into historical data,
    filters out the data of the specified user from the test data, and converts it into the input and target data
    required for prediction.

    Args:
        dataset_name (str): The name of the dataset.
        user_train (pd.DataFrame): The user's training data, usually a Pandas DataFrame.
        test_dataset (list): The test dataset, which is a list containing multiple dictionaries.
        uid (int or str): The unique identifier of the user.
        logger (logging.Logger): The logger object used to record information.
        Recent_Trajectory (int): The number of recent trajectories, with a default value of 10.

    Returns:
        tuple: A tuple containing three elements: a list of historical data, a list of prediction input data,
               and a list of prediction target data.
    """
    # Organize data in another way
    # Initialize the list for historical data
    No_recent_Trajectory_Points_Data = []
    # Iterate through each row of the user's training data
    for _, row in user_train.iterrows():
        # Convert each row of data into a specific format and add it to the historical data list
        No_recent_Trajectory_Points_Data.append(
            (convert_minutes_to_12_hour_format(int(row['start_min'])),
             integer_to_weekday(row['weekday']),
             int(row['duration']),
             row['location_id'])
        )

    # Log information about historical data
    logger.info(f"No_recent_Trajectory_Points_Data: {No_recent_Trajectory_Points_Data}")
    logger.info(f"Number of No_recent_Trajectory_Points_Data: {len(No_recent_Trajectory_Points_Data)}")

    # Get the i-th test data of the user
    # Initialize the list of user test data dictionaries
    list_user_dict = []
    # Iterate through the test dataset
    for i_dict in test_dataset:
        # Get the user ID of the current test data
        i_uid = i_dict['user_X'][0]
        # If the user ID matches the specified user ID, add the test data to the list
        if i_uid == uid:
            list_user_dict.append(i_dict)

    # Initialize the lists for prediction input data and prediction target data
    predict_X = []
    predict_y = []
    # Iterate through the list of user test data dictionaries
    for i_dict in list_user_dict:
        # Initialize the dictionary for constructing prediction input
        construct_dict = {}
        # Construct context data, including recent start times, weekdays, durations, and location information
        context = list(
            zip([convert_minutes_to_12_hour_format(int(item)) for item in i_dict['start_min_X'][-Recent_Trajectory:]],
                [integer_to_weekday(i) for i in i_dict['weekday_X'][-Recent_Trajectory:]],
                [int(i) for i in i_dict['dur_X'][-Recent_Trajectory:]],
                i_dict['X'][-Recent_Trajectory:]))
        # Construct target data, including target start time, weekday, duration (set to None), and placeholder
        target = (
            convert_minutes_to_12_hour_format(int(i_dict['start_min_Y'])), integer_to_weekday(i_dict['weekday_Y']), None, "<next_place_id>")
        # Add context data and target data to the construction dictionary
        construct_dict['Recent_Trajectory_Points_Data'] = context
        construct_dict['Target_stay'] = target
        # Add target data to the prediction target data list
        predict_y.append(i_dict['Y'])
        # Add the construction dictionary to the prediction input data list
        predict_X.append(construct_dict)

    # Log information about prediction data
    # logger.info(f"predict_data: {predict_X}")
    logger.info(f"Number of predict_data: {len(predict_X)}")
    logger.info(f"predict_y: {predict_y}")
    logger.info(f"Number of predict_y: {len(predict_y)}")
    # Return historical data, prediction input data, and prediction target data
    return No_recent_Trajectory_Points_Data, predict_X, predict_y

def single_query_num_10(client, No_recent_Trajectory_Points_Data, X):
    """
    Make a single query of 10 most likely places
    param:
    X: one single sample containing Recent Trajectory and Target stay
    """
    prompt = f"""
            You need to predict the user's next location based on their activity patterns.The first input is a list named <No-recent Trajectory Points>, which records the aggregated locations the user has visited over a continuous period in the past. The second input is <Recent Trajectory Points>, which provides the aggregated locations the user has visited over a recent continuous period.Based on the following information, infer the <Next Future Trajectory Point FT>, which is the location the user is most likely to arrivate next.First, analyze the user's activity patterns presented in the <No-recent Trajectory Points> data. Observe whether the user has a recurring behavior of visiting a specific location at a particular time.Second, refer to the activity patterns in the <Recent Trajectory Points> data, which reflect the user's recent interest in certain trajectories and provide more immediate information for your inference.Finally, pay attention to the duration in each trajectory point, as it indicates the user's activity level at certain locations.After completing the inference, organize the answer into a JSON-formatted object. The object should contain a key "prediction": the corresponding value is the location PD you inferred.
            Relevant data is as follows:
            <No-recent Trajectory Points>: {No_recent_Trajectory_Points_Data}
            <Recent Trajectory Points>: {X['Recent_Trajectory_Points_Data']}
            Note: The user's location records follow a specific format: (ST, DT, LT, PD). The explanation for each component is as follows:
            ST: Start Time, using a 12-hour format, clearly records the time the user started traveling to this location.
            DT: Day of the Week, clearly indicates the specific day of the week when the user traveled to this location.
            LT: Duration, presented as an integer in minutes, represents the duration of the user's travel at this location.
            PD: Location ID, a unique integer identifier for the user's trajectory point, which can precisely locate the specific position.
            """
    completion = get_chat_completion(client, prompt)
    return completion

def load_results(filename):
    # Load previously saved results from a CSV file
    results = pd.read_csv(filename)
    return results

def single_user_query(client, dataset_name, uid, No_recent_Trajectory_Points_Data, predict_X, predict_y, logger, num_k, output):
    """
    This function is used to perform single-user queries. It processes prediction data for a specific user,
    makes queries to an API, logs the results, and saves the results to a CSV file. If the query is interrupted,
    it can resume from the last successful point.

    Args:
        client: The client object used to make API requests.
        dataset_name (str): The name of the dataset.
        uid: The unique identifier of the user.
        No_recent_Trajectory_Points_Data: Data related to non-recent trajectory points.
        predict_X (list): A list of prediction input data.
        predict_y (list): A list of prediction target data.
        logger: The logger object used to record information.
        num_k (int): A parameter related to the number of predictions.
        output (str): The output directory where the results will be saved.

    Returns:
        None
    """
    # Initialize variables
    # Calculate the total number of queries
    total_queries = len(predict_X)
    logger.info(f"Total_queries: {total_queries}")

    # Initialize the number of processed queries
    processed_queries = 0
    # Initialize a DataFrame to store current results
    current_results = pd.DataFrame({
        'user_id': None,
        'ground_truth': None,
        'prediction': None,
    }, index=[])

    # Generate the output file name
    out_filename = f"{uid:02d}" + ".csv"
    # Generate the output file path
    out_filepath = os.path.join(output, out_filename)

    try:
        # Attempt to load previous results if the file exists
        current_results = load_results(out_filepath)
        # Update the number of processed queries
        processed_queries = len(current_results)
        logger.info(f"Loaded {processed_queries} previous results.")
    except FileNotFoundError:
        logger.info("No previous results found. Starting from scratch.")

    # Process remaining queries
    # Iterate through the remaining queries
    for i in range(processed_queries, total_queries):
        logger.info(f'The {i + 1}th sample: ')
        # Uncomment the following two lines if you want to log context and target stay information
        # logger.info(f"context: {predict_X[i]['context_stay']}")
        # logger.info(f"target stay: {predict_X[i]['target_stay']}")

        # Make a single query based on the dataset name
        if dataset_name == 'geolife':
            completions = single_query_num_10(client, No_recent_Trajectory_Points_Data, predict_X[i])

        # Get the response content from the API
        response = completions.choices[0].message.content

        # Log the prediction results and usage information
        logger.info(f"Pred results: {response}")
        logger.info(f"Ground truth: {predict_y[i]}")
        logger.info(dict(completions).get('usage'))

        try:
            # Convert the response string to a dictionary
            res_dict = ast.literal_eval(response)
            # Format the prediction if num_k is not 1
            if num_k != 1:
                res_dict['prediction'] = str(res_dict['prediction'])
            # Add user ID and ground truth to the result dictionary
            res_dict['user_id'] = uid
            res_dict['ground_truth'] = predict_y[i]
        except Exception as e:
            # If conversion fails, create a default result dictionary
            res_dict = {'user_id': uid, 'ground_truth': predict_y[i], 'prediction': -100}
            logger.info(e)
            logger.info(f"API request failed for the {i + 1}th query")
            # Uncomment the following line if you want to add a delay after a failed request
            # time.sleep(sleep_crash)
        finally:
            # Create a new DataFrame row from the result dictionary
            new_row = pd.DataFrame(res_dict, index=[0])
            # Concatenate the new row to the current results DataFrame
            current_results = pd.concat([current_results, new_row], ignore_index=True)

    # Save the current results to a CSV file
    current_results.to_csv(out_filepath, index=False)
    # You can also use a custom save_results function if available
    # save_results(current_results, out_filename)
    logger.info(f"Saved {len(current_results)} results to {out_filepath}")

    # If there are still remaining queries, restart the query process
    if len(current_results) < total_queries:
        logger.info("Restarting queries from the last successful point.")
        single_user_query(client, dataset_name, uid, No_recent_Trajectory_Points_Data, predict_X, predict_y,
                          logger, num_k, output)

def query_all_user(client, dataset_name, uid_list, logger, train_dataset, No_Recent_Trajectory,
                   Recent_Trajectory, test_dataset, num_k, output):
    """
    This function is used to query data for all users in the given user ID list.
    It iterates through each user ID, retrieves the user's training data, organizes the data,
    and then performs a single - user query.

    Args:
        client: The client object used for querying, typically an API client.
        dataset_name (str): The name of the dataset being used.
        uid_list (list): A list containing the unique identifiers (UIDs) of all users to be queried.
        logger: The logger object used for logging information about the query process.
        train_dataset: The training dataset containing data for all users.
        No_Recent_Trajectory: A parameter related to non-recent trajectory data.
        Recent_Trajectory: A parameter related to recent trajectory data.
        test_dataset: The test dataset containing test data for all users.
        num_k (int): A parameter specifying the number of num_k results (used in prediction).
        output (str): The directory path where the query results will be saved.

    Returns:
        None
    """
    # Iterate through each user ID in the list
    for uid in uid_list:
        # Log the start of processing for the current user
        logger.info(f"=================Processing user {uid}==================")

        # Retrieve the training data for the current user
        user_train = get_user_data(train_dataset, uid, No_Recent_Trajectory, logger)

        # Organize the data for the current user, including historical data and prediction data
        No_recent_Trajectory_Points_Data, predict_X, predict_y = organise_data(dataset_name, user_train, test_dataset, uid, logger,
                                                                              Recent_Trajectory)

        # Perform a single - user query for the current user
        single_user_query(client, dataset_name, uid, No_recent_Trajectory_Points_Data, predict_X, predict_y, logger, num_k=num_k, output=output)

def get_unqueried_user(dataset_name, output='output/'):
    """
    This function is used to find the IDs of users who have not been queried.
    It first checks if the output directory exists, and creates it if not.
    Then, based on the dataset name, it determines all possible user IDs.
    After that, it identifies the processed user IDs by looking at the CSV files in the output directory.
    Finally, it returns the IDs of the users who have not been processed.

    Args:
        dataset_name (str): The name of the dataset.
        output (str): The output directory where the query results are stored. Defaults to 'output/'.

    Returns:
        list: A list of user IDs that have not been queried.
    """
    # Check if the output directory exists, and create it if not
    if not os.path.exists(output):
        os.makedirs(output)

    # Determine all possible user IDs based on the dataset name
    if dataset_name == "geolife":
        all_user_id = [i + 1 for i in range(45)]

    # Get the IDs of the processed users by looking at the CSV files in the output directory
    processed_id = [int(file.split('.')[0]) for file in os.listdir(output) if file.endswith('.csv')]

    # Find the IDs of the users who have not been processed
    remain_id = [i for i in all_user_id if i not in processed_id]

    # Print the remaining user IDs and the number of remaining user IDs
    print(remain_id)
    print(f"Number of the remaining id: {len(remain_id)}")

    # Return the IDs of the users who have not been processed
    return remain_id

if __name__ == "__main__":
    API_KEY = "YOURS"
    BASE_URL = "YOURS"
    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    dataset_name = "geolife"
    No_Recent_Trajectory = 50
    Recent_Trajectory = 10
    num_k = 10
    output = f"output/{dataset_name}/num_10"
    log = f"logs/{dataset_name}/num_10"
    tv_dataset, test_dataset = load_dataset(dataset_name)
    logger = setup_logger('logger', log=log)
    uid_list = get_unqueried_user(dataset_name, output)
    print(f"uid_list: {uid_list}")
    query_all_user(client, dataset_name, uid_list, logger, tv_dataset, No_Recent_Trajectory, Recent_Trajectory,
                   test_dataset, output=output, num_k=num_k)